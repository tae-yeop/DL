{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy_based.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPO5G0y+bqZQpLiAeeZZWSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae-yeop/DL/blob/main/Policy_based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4w0P-rfKWAf"
      },
      "source": [
        "## REINFORCE\n",
        "\n",
        "__Algorithm__\n",
        "\n",
        "Init a random polity $\\pi_{\\theta}(a; s)$, using the policy, we collect a trajectory $ s_1, a_1, r_1, s_2, a_2, r_2, ...$.\n",
        "\n",
        "compute the total reward of the trajectory $R = r_1 + r_2 + r_3 + ... $\n",
        "\n",
        "estimate the gradient of the expected reward \n",
        "$ g = R \\sum_{t} \\nabla_{\\theta} \\log{\\pi_{\\theta} (a_t | s_t)} $\n",
        "\n",
        "update the policy suing gradient ascent\n",
        "$ \\theta \\leftarrow \\theta + \\alpha g$\n",
        "\n",
        "\n",
        "__Issues__\n",
        "\n",
        "- The update process is very inefficient\n",
        "  - run the policy once, update once, the throw away the trajectory\n",
        "\n",
        "- gradient "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAf2TnLsZilB"
      },
      "source": [
        "## PPO(Proximal Policy Optimization) \n",
        "<h3 align=\"center\">This is a centered header</h3>\n",
        "\n",
        "## <center>Your centered level h2 title</center>\n",
        "\n",
        "\n",
        "_Noise Reduction_\n",
        "\n",
        "_Credit Assignment_\n",
        "\n",
        "Even before an action is decided, the agent has already recevied all the reward up until step $ t-1$.\n",
        "$$ g = \\sum_{t} (... + r_{t-1} + r_{t} +...) \\nabla_{\\theta} \\log{\\pi_{\\theta}(a_t|s_t)} $$\n",
        "\n",
        "$$ \n",
        "\\begin{equation*} \n",
        "  R = \\begin{cases}\n",
        "        ... + r_{t-1}    & R_{t}^{\\text{past}}\\\\\n",
        "        r_{t} + ...    & R_{t}^{\\text{future}}\n",
        "      \\end{cases}\n",
        "\\end{equation*} \n",
        "$$\n",
        "\n",
        "Because we have a Markov proceess, the actiona at time step $t$ can only affect the future reward, so the past reward shouldn't contribute to the policy gradient. Therefore ignore the past reward.\n",
        "$$ g = \\sum_{t} R_{t}^{\\text{futre}} \\nabla_{\\theta} \\log{\\pi_{\\theta}(a_t|s_t)} $$\n",
        "\n",
        "\n",
        "_Gradient Modification_\n",
        "\n",
        "Ignoring past rewards might change the gradient for each trajectory, but it doesn't change the averaged gradient. On average we still maximizaing the average reward. Using past reward speed up training with less noisy gradient.\n",
        "\n",
        "\n",
        "_Quiz_\n",
        "\n",
        "$$ \\sum_{t}\n",
        "\n",
        "\n",
        "_Importance Sampling(Recycling Stratedgy)_\n",
        "\n",
        "We generate the trajectory based on the policy. Thereofre Trajectories are representation of current policy. These trajectories are hard to generate and might be expensive.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1quGmyiokHxtRLmeJem--Xh0hKIGDNibl' />\n",
        "<img src='https://drive.google.com/file/d/1quGmyiokHxtRLmeJem--Xh0hKIGDNibl/view?usp=sharing'/>\n",
        "\n",
        "\n",
        "Instead of throwing away trajectoies, modify them to make them representative of the new policy. Then use it to update the poilcy.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1huC2dviJRjPUsrd2LxAtpSnEyPTStsWC' />\n",
        "<img src='https://drive.google.com/file/d/1huC2dviJRjPUsrd2LxAtpSnEyPTStsWC/view?usp=sharing'/>\n",
        "\n",
        "새로운 policy에서 어떤 평균의 quantity를 계산하려고 함.\n",
        "$$ \\sum_{\\tau} P(\\tau ; \\theta')f(\\tau) $$\n",
        "$$ \\sum_{\\tau} P(\\tau ; \\theta') \\frac{P(\\tau ; \\theta)}{P(\\tau ; \\theta)} f(\\tau) \\\\ $$\n",
        "$$ = \\sum_{\\tau} P(\\tau ; \\theta) \\frac{P(\\tau ; \\theta')}{P(\\tau ; \\theta)} f(\\tau) \\\\  $$\n",
        "$$ P(\\tau ; \\theta) : \\text{old policy} $$\n",
        "$$ \\frac{P(\\tau ; \\theta')}{P(\\tau ; \\theta)} : \\text{Re-weight factor} $$\n",
        "\n",
        "Re-weighting factor\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=105mytYwMA2pg9EpMUOMxdXSWVA_VsMrY' />\n",
        "<img src='https://drive.google.com/file/d/105mytYwMA2pg9EpMUOMxdXSWVA_VsMrY/view?usp=sharing'/>\n",
        "\n",
        "각각 trajectory는 많은 step을 가지고 있고 이는 policy들의 chain rule로 표현됨.\n",
        "When some policy gets close to zero, the re-weighting factor can become close to zero, or worse, close to 1 over 0 which diverges to infinity.\n",
        "\n",
        "Who this happens, the re-weighting trick becomes unrelialbe. So, In practice, make sure the factor is not too far from 1.\n",
        "\n",
        "### The Surrogate function###\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINGa7mymO2S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}